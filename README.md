
conda create --name wikibase_cortex_env_0410 python=3.10 -y

pip install -r requirements.txt

Project Name : wikibase-cortex-assistant
FLOW :

User sends a question ‚Üí FastAPI receives it ‚Üí Embeds query using Snowflake Cortex ‚Üí Retrieves top matching Wikipedia chunks from Snowflake using vector search ‚Üí Sends context + question to Cortex LLM ‚Üí Returns generated answer to user.


User sends a question
    ‚Üì
FastAPI receives it (via endpoint in `api/main.py`)
    ‚Üì
Query is embedded using Snowflake Cortex (handled in `cortex_handler.py`)
    ‚Üì
Vector search in Snowflake retrieves top-matching Wikipedia chunks (via `snowflake_client.py`)
    ‚Üì
Relevant chunks + original question sent to Cortex LLM
    ‚Üì
Cortex generates an answer
    ‚Üì
FastAPI returns the answer to the user




### ‚úÖ **Embedding Flow in Snowflake Cortex (Query ‚Üí Vector)**

Snowflake provides a built-in function for embeddings:

#### **Model Used for Embeddings:**
You‚Äôll use:
```sql
CORTEX.EMBED_TEXT(<'text'> USING PARAMETERS MODEL_NAME = '<embedding-model>')
```

Example embedding call:
```sql
SELECT cortex.embed_text('Who is Marie Curie?' USING PARAMETERS MODEL_NAME = 'snowflake/snowflake-arctic-embed');
```

#### ‚úÖ Recommended Embedding Model:
- `"snowflake/snowflake-arctic-embed"` (Optimized for RAG tasks, supported natively in Snowflake)
- Or: `"snowflake/embedding-ada"` (OpenAI Ada-like, if supported)

---

### üîÅ **How it Fits in the RAG Flow**

| Step                        | Detail                                                                 |
|-----------------------------|------------------------------------------------------------------------|
| 1. **User query**           | e.g., "Tell me about Newton‚Äôs laws"                                    |
| 2. **Embed query**          | Use `CORTEX.EMBED_TEXT` to convert query to vector                     |
| 3. **Search DB**            | Run **vector similarity search** on pre-embedded Wikipedia chunks      |
| 4. **Retrieve top-k**       | Select top matching chunks (e.g., via cosine distance)                 |
| 5. **Send to LLM**          | Combine question + retrieved context ‚Üí send to `CORTEX.COMPLETE()`     |
| 6. **Generate answer**      | LLM generates final answer with better grounding/context               |

---

### üß† Summary
You are:
- Using **Cortex‚Äôs embedding models** (no local model needed).
- Relying on **Snowflake-managed embedding + vector search**, fully serverless.
- Supporting a clean **RAG architecture** with:
  - **Chunked documents** stored in Snowflake
  - **Query vector** matched against stored vectors
  - **Final answer** generated using LLM with context

User ‚Üí FastAPI ‚Üí Cortex Embed ‚Üí Snowflake Vector Search ‚Üí Context + Query ‚Üí Cortex LLM ‚Üí Answer ‚Üí User

docker build -t streamlit-rag .